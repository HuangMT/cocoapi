{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "import os\n",
    "from io import BytesIO\n",
    "import tarfile\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "_ROOT = '/home/mingtao.huang/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_TYPE = 'owlii'\n",
    "\n",
    "root_path = '/home/mingtao.huang/'\n",
    "model_dir = '/home/mingtao.huang/log_repo/deploy/'\n",
    "res_dir = model_dir + 'res/'\n",
    "if not os.path.exists(res_dir):\n",
    "  os.makedirs(res_dir)\n",
    "\n",
    "FROZEN_GRAPH_NAME = model_dir + 'mobilenet_v3_035_deploy.pb'\n",
    "#FROZEN_GRAPH_NAME = '/home/mingtao.huang/deploy_graph.pb'\n",
    "INPUT_SIZE = 513\n",
    "\n",
    "class SgmtModel(object):\n",
    "  \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "  INPUT_TENSOR_NAME = 'image:0'\n",
    "  OUTPUT_TENSOR_NAME = 'heatmap:0'\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "    self.graph = tf.Graph()\n",
    "    graph_def = None\n",
    "    with tf.gfile.GFile(FROZEN_GRAPH_NAME, \"rb\") as f:\n",
    "      #print(FROZEN_GRAPH_NAME)\n",
    "      graph_def = tf.GraphDef().FromString(f.read())\n",
    "\n",
    "    if graph_def is None:\n",
    "      raise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "\n",
    "    with self.graph.as_default():\n",
    "      tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    self.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "  def run(self, image):\n",
    "    \"\"\"Runs inference on a single image.\n",
    "\n",
    "    Args:\n",
    "      image: A PIL.Image object, raw input image.\n",
    "\n",
    "    Returns:\n",
    "      resized_image: RGB image resized from original input image.\n",
    "      seg_map: Segmentation map of `resized_image`.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    resize_ratio = 1.0 * INPUT_SIZE / max(width, height)\n",
    "    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "    the_input = [np.asarray(resized_image)]\n",
    "\n",
    "    t1 = time.time()\n",
    "    heatmap = self.sess.run(\n",
    "        self.OUTPUT_TENSOR_NAME,\n",
    "        feed_dict={self.INPUT_TENSOR_NAME: the_input})\n",
    "    t2 = time.time()\n",
    "    total = (t2 - t1) * 1000\n",
    "\n",
    "    heatmap = heatmap[0, :, :, :]\n",
    "\n",
    "    return heatmap, total\n",
    "\n",
    "MODEL = SgmtModel()\n",
    "def infer_one(image, use_heatmap=True):\n",
    "  # image preprocessing\n",
    "#  img = Image.open(image_path)\n",
    "  width, height = image.size\n",
    "  large_one = max(width, height)\n",
    "  \n",
    "  scale = float(INPUT_SIZE) / float(large_one)\n",
    "  \n",
    "  new_width = 0\n",
    "  new_height = 0\n",
    "  if width >= height:\n",
    "    new_width = INPUT_SIZE\n",
    "    new_height = int(height * scale)\n",
    "  else:\n",
    "    new_height = INPUT_SIZE\n",
    "    new_width = int(width * scale)\n",
    "  \n",
    "  image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "  # padding\n",
    "  delta_w = INPUT_SIZE - new_width\n",
    "  delta_h = INPUT_SIZE - new_height\n",
    "  top, bottom = int(delta_h / 2), int(delta_h) - int(delta_h / 2)\n",
    "  left, right = int(delta_w / 2), int(delta_w) - int(delta_w / 2)\n",
    "  color = [127, 127, 127]\n",
    "  img_array = np.array(image)\n",
    "  img_array = cv2.copyMakeBorder(img_array, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "      value=color)\n",
    "  \n",
    "  image = Image.fromarray(np.uint8(img_array))\n",
    "  # run model\n",
    "  \n",
    "  heatmap, running_time = MODEL.run(image)\n",
    "  heatmap = np.float32(heatmap) / 255.0\n",
    "  heatmap = cv2.resize(heatmap,(INPUT_SIZE,INPUT_SIZE)).reshape((INPUT_SIZE,INPUT_SIZE,1))\n",
    "  if not use_heatmap:\n",
    "    heatmap = np.where(heatmap > 0.5, 1, 0)\n",
    "  # post processing\n",
    "  embed_array = img_array\n",
    "  embed_array = np.multiply(img_array, heatmap) \n",
    "  \n",
    "  # get results\n",
    "  embed_crop = embed_array[int(delta_h / 2) : int(new_height + delta_h / 2), int(delta_w / 2) : int(new_width + delta_w / 2)]\n",
    "  embed_crop = Image.fromarray(np.uint8(embed_crop))\n",
    "#  embed_crop.save('data/embed_tf.png')\n",
    "  heatmap_array = np.squeeze(heatmap)\n",
    "  if not use_heatmap:\n",
    "    heatmap_array = heatmap_array>0.5\n",
    "  else:\n",
    "    heatmap_array = heatmap_array[int(delta_h / 2) : int(new_height + delta_h / 2), int(delta_w / 2) : int(new_width + delta_w / 2)]*255\n",
    "    \n",
    "  heatmap_crop = Image.fromarray(np.uint8(heatmap_array))\n",
    "#  heatmap_crop.save('data/heatmap_tf.png')\n",
    "\n",
    "  return embed_crop, heatmap_crop, running_time\n",
    "\n",
    "#image = Image.open(root_path + 'test2.jpg')\n",
    "#embed_crop, heatmap_crop, running_time,heat_map = infer_one(image, True)\n",
    "\n",
    "\n",
    "'''\n",
    "# now start inferring\n",
    "with open(listpath) as f:\n",
    "    lines = f.readlines()\n",
    "lines = [x.strip('\\n') for x in lines] \n",
    "#print(lines)\n",
    "\n",
    "for filename in lines:\n",
    "  filename_root = os.path.splitext(filename)[0]\n",
    "\n",
    "  image = Image.open(data_dir + filename)\n",
    "  embed_crop, heatmap_crop, running_time = infer_one(image, False)\n",
    "  \n",
    "  image.save(res_dir + filename)\n",
    "  embed_crop.save(res_dir + filename_root + '.embed_tf.png')\n",
    "  heatmap_crop.save(res_dir + filename_root + '.heatmap_tf.png')\n",
    "  print('Time consumed on ', filename, ': ', running_time, ' ms.')\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "root_path = '/home/mingtao.huang/'\n",
    "data_path = root_path + 'dataset/coco/'\n",
    "image_path = data_path + 'train2017/'\n",
    "mask_path = data_path + 'mask_train2017/'\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for name in os.listdir(image_path):\n",
    "    cnt += 1\n",
    "    if cnt > 10:\n",
    "        break\n",
    "    path = image_path + name\n",
    "    image = Image.open(path)\n",
    "    embed_crop, heatmap_crop, running_time = infer_one(image, False)\n",
    "    #cv2.imwrite(mask_path + name.split('.')[0]+'.png',embed_crop)\n",
    "    heatmap_crop.save(mask_path + name.split('.')[0]+'.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_pseudo_mask.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_pseudo_mask.py\n",
    "\n",
    "import os\n",
    "from io import BytesIO\n",
    "import tarfile\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "_ROOT = '/home/mingtao.huang/'\n",
    "\n",
    "MODEL_TYPE = 'owlii'\n",
    "\n",
    "root_path = '/home/mingtao.huang/'\n",
    "model_dir = '/home/mingtao.huang/log_repo/deploy/'\n",
    "res_dir = model_dir + 'res/'\n",
    "if not os.path.exists(res_dir):\n",
    "  os.makedirs(res_dir)\n",
    "\n",
    "FROZEN_GRAPH_NAME = model_dir + 'mobilenet_v3_035_deploy.pb'\n",
    "#FROZEN_GRAPH_NAME = '/home/mingtao.huang/deploy_graph.pb'\n",
    "INPUT_SIZE = 513\n",
    "\n",
    "class SgmtModel(object):\n",
    "  \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "  INPUT_TENSOR_NAME = 'image:0'\n",
    "  OUTPUT_TENSOR_NAME = 'heatmap:0'\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "    self.graph = tf.Graph()\n",
    "    graph_def = None\n",
    "    with tf.gfile.GFile(FROZEN_GRAPH_NAME, \"rb\") as f:\n",
    "      #print(FROZEN_GRAPH_NAME)\n",
    "      graph_def = tf.GraphDef().FromString(f.read())\n",
    "\n",
    "    if graph_def is None:\n",
    "      raise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "\n",
    "    with self.graph.as_default():\n",
    "      tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    self.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "  def run(self, image):\n",
    "    \"\"\"Runs inference on a single image.\n",
    "\n",
    "    Args:\n",
    "      image: A PIL.Image object, raw input image.\n",
    "\n",
    "    Returns:\n",
    "      resized_image: RGB image resized from original input image.\n",
    "      seg_map: Segmentation map of `resized_image`.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    resize_ratio = 1.0 * INPUT_SIZE / max(width, height)\n",
    "    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "    the_input = [np.asarray(resized_image)]\n",
    "\n",
    "    t1 = time.time()\n",
    "    heatmap = self.sess.run(\n",
    "        self.OUTPUT_TENSOR_NAME,\n",
    "        feed_dict={self.INPUT_TENSOR_NAME: the_input})\n",
    "    t2 = time.time()\n",
    "    total = (t2 - t1) * 1000\n",
    "\n",
    "    heatmap = heatmap[0, :, :, :]\n",
    "\n",
    "    return heatmap, total\n",
    "\n",
    "MODEL = SgmtModel()\n",
    "def infer_one(image, use_heatmap=True):\n",
    "  # image preprocessing\n",
    "#  img = Image.open(image_path)\n",
    "  width, height = image.size\n",
    "  large_one = max(width, height)\n",
    "  \n",
    "  scale = float(INPUT_SIZE) / float(large_one)\n",
    "  \n",
    "  new_width = 0\n",
    "  new_height = 0\n",
    "  if width >= height:\n",
    "    new_width = INPUT_SIZE\n",
    "    new_height = int(height * scale)\n",
    "  else:\n",
    "    new_height = INPUT_SIZE\n",
    "    new_width = int(width * scale)\n",
    "  \n",
    "  image = image.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "  # padding\n",
    "  delta_w = INPUT_SIZE - new_width\n",
    "  delta_h = INPUT_SIZE - new_height\n",
    "  top, bottom = int(delta_h / 2), int(delta_h) - int(delta_h / 2)\n",
    "  left, right = int(delta_w / 2), int(delta_w) - int(delta_w / 2)\n",
    "  color = [127, 127, 127]\n",
    "  img_array = np.array(image)\n",
    "  img_array = cv2.copyMakeBorder(img_array, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "      value=color)\n",
    "  \n",
    "  image = Image.fromarray(np.uint8(img_array))\n",
    "  # run model\n",
    "  \n",
    "  heatmap, running_time = MODEL.run(image)\n",
    "  heatmap = np.float32(heatmap) / 255.0\n",
    "  heatmap = cv2.resize(heatmap,(INPUT_SIZE,INPUT_SIZE)).reshape((INPUT_SIZE,INPUT_SIZE,1))\n",
    "  if not use_heatmap:\n",
    "    heatmap = np.where(heatmap > 0.5, 1, 0)\n",
    "    \n",
    "  heatmap_array = np.squeeze(heatmap)\n",
    "  if not use_heatmap:\n",
    "    heatmap_array = heatmap_array>0.5\n",
    "  else:\n",
    "    heatmap_array = heatmap_array[int(delta_h / 2) : int(new_height + delta_h / 2), int(delta_w / 2) : int(new_width + delta_w / 2)]*255\n",
    "    \n",
    "  heatmap_crop = Image.fromarray(np.uint8(heatmap_array))\n",
    "#  heatmap_crop.save('data/heatmap_tf.png')\n",
    "\n",
    "  return heatmap_crop\n",
    "\n",
    "#image = Image.open(root_path + 'test2.jpg')\n",
    "#embed_crop, heatmap_crop, running_time,heat_map = infer_one(image, True)\n",
    "\n",
    "\n",
    "'''\n",
    "# now start inferring\n",
    "with open(listpath) as f:\n",
    "    lines = f.readlines()\n",
    "lines = [x.strip('\\n') for x in lines] \n",
    "#print(lines)\n",
    "\n",
    "for filename in lines:\n",
    "  filename_root = os.path.splitext(filename)[0]\n",
    "\n",
    "  image = Image.open(data_dir + filename)\n",
    "  embed_crop, heatmap_crop, running_time = infer_one(image, False)\n",
    "  \n",
    "  image.save(res_dir + filename)\n",
    "  embed_crop.save(res_dir + filename_root + '.embed_tf.png')\n",
    "  heatmap_crop.save(res_dir + filename_root + '.heatmap_tf.png')\n",
    "  print('Time consumed on ', filename, ': ', running_time, ' ms.')\n",
    "'''\n",
    "\n",
    "root_path = '/home/mingtao.huang/'\n",
    "data_path = root_path + 'dataset/coco/'\n",
    "image_path = data_path + 'train2017/'\n",
    "mask_path = data_path + 'mask_train2017/'\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for name in os.listdir(image_path):\n",
    "    cnt += 1\n",
    "    if cnt % 100 == 0:\n",
    "        print(cnt)\n",
    "    path = image_path + name\n",
    "    image = Image.open(path)\n",
    "    heatmap_crop = infer_one(image, False)\n",
    "    #cv2.imwrite(mask_path + name.split('.')[0]+'.png',embed_crop)\n",
    "    heatmap_crop.save(mask_path + name.split('.')[0]+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
